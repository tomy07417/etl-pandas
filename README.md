# ğŸ“Š Pipeline ETL de E-commerce

> Un sistema automatizado de extracciÃ³n, transformaciÃ³n y carga de datos que redujo el tiempo de generaciÃ³n de reportes de **2 horas a 3 minutos**.

[![Python](https://img.shields.io/badge/Python-3.8+-blue.svg)](https://www.python.org/downloads/)
[![Pandas](https://img.shields.io/badge/Pandas-1.3+-green.svg)](https://pandas.pydata.org/)
[![License](https://img.shields.io/badge/License-MIT-yellow.svg)](LICENSE)

---

## ğŸ¯ DescripciÃ³n del Proyecto

Este proyecto implementa un **pipeline ETL completo** que procesa datos de e-commerce simulando un entorno empresarial real. El sistema automatiza la extracciÃ³n, limpieza, transformaciÃ³n y almacenamiento de datos desde mÃºltiples fuentes relacionales, generando insights accionables para el equipo de negocio.

### ğŸš€ Problema Resuelto

Antes de la implementaciÃ³n, el equipo de negocio dedicaba **2 horas diarias** a generar reportes manualmente en Excel, consolidando datos de mÃ¡s de 10 tablas. Este proyecto automatizÃ³ completamente ese proceso.

---

## ğŸ’¡ CaracterÃ­sticas Principales

- âœ… **Procesamiento automatizado** de 10+ tablas relacionadas (Ã³rdenes, productos, clientes, inventario)
- âœ… **Limpieza de datos inteligente** con manejo contextual de valores nulos
- âœ… **CÃ¡lculo de mÃ©tricas de negocio** (top clientes, productos estrella, tendencias)
- âœ… **OptimizaciÃ³n de almacenamiento** con formato Parquet (reducciÃ³n 8x vs CSV)
- âœ… **Pipeline ejecutable** en 3 minutos end-to-end

---

## ğŸ—ï¸ Arquitectura

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Raw Data (CSV) â”‚
â”‚  10+ tablas     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   EXTRACTION    â”‚
â”‚  Pandas Load    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ TRANSFORMATION  â”‚
â”‚ â€¢ Data Quality  â”‚
â”‚ â€¢ Cleaning      â”‚
â”‚ â€¢ Calculations  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚     LOADING     â”‚
â”‚ Parquet Format  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ”§ Stack TecnolÃ³gico

| TecnologÃ­a | Uso |
|-----------|-----|
| **Python 3.8+** | Lenguaje principal |
| **Pandas** | ManipulaciÃ³n y anÃ¡lisis de datos |
| **NumPy** | Operaciones numÃ©ricas optimizadas |
| **Parquet** | Almacenamiento columnar eficiente |
| **Jupyter Notebooks** | ExploraciÃ³n y desarrollo |
| **AWS S3** | Almacenamiento en la nube |
| **Boto3** | SDK de AWS para Python |

---

## ğŸ“ˆ Resultados Cuantificables

| MÃ©trica | Antes | DespuÃ©s | Mejora |
|---------|-------|---------|--------|
| **Tiempo de proceso** | 2 horas (manual) | 3 minutos (automÃ¡tico) | **40x mÃ¡s rÃ¡pido** |
| **TamaÃ±o de archivos** | 2.4 MB (CSV) | 300 KB (Parquet) | **8x reducciÃ³n** |
| **Errores humanos** | Frecuentes | Eliminados | **100% precisiÃ³n** |
| **Frecuencia de reportes** | Semanal | Diario | **7x mÃ¡s frecuente** |

---

## ğŸ“ Decisiones TÃ©cnicas Clave

### 1ï¸âƒ£ Manejo de Valores Nulos

**Problema:** La columna `parent_category_id` en categories tenÃ­a todos sus valores nulos.

**SoluciÃ³n:** EliminaciÃ³n de la columna `parent_category_id` ya que no aportaba informaciÃ³n Ãºtil.

**RazÃ³n:** Una columna completamente vacÃ­a no proporciona valor analÃ­tico y solo ocupa espacio.

```python
df_categories.drop(columns=['parent_category_id'], inplace=True)
```

**Otros valores nulos identificados:**
- `orders.promotion_id`: NaN indica que la orden no tiene promociÃ³n aplicada (se mantiene).
- `orders.notes`: NaN indica ausencia de notas en la orden (se mantiene).

### 2ï¸âƒ£ VerificaciÃ³n de Duplicados

**AnÃ¡lisis:** Se verificÃ³ la existencia de duplicados en todas las tablas.

**Resultado:** No se encontraron registros duplicados en ninguna tabla.

**Importancia:** Validar la ausencia de duplicados garantiza la integridad de las mÃ©tricas calculadas.

### 3ï¸âƒ£ Formato Parquet vs CSV

**DecisiÃ³n:** Almacenamiento dual en CSV y Parquet para flexibilidad.

**Beneficios de Parquet:**
- ğŸ—œï¸ CompresiÃ³n columnar (8x reducciÃ³n)
- âš¡ Lectura mÃ¡s rÃ¡pida (solo carga columnas necesarias)
- ğŸ¯ PreservaciÃ³n de tipos de datos
- ğŸ“Š Compatible con herramientas Big Data (Spark, Hive)

### 4ï¸âƒ£ IntegraciÃ³n con AWS S3

**DecisiÃ³n:** Implementar carga de datos a AWS S3 para almacenamiento en la nube.

**ImplementaciÃ³n:**
- MÃ³dulo `s3.py` con funciones reutilizables para lectura y escritura
- ConfiguraciÃ³n segura mediante variables de entorno (`.env`)
- Soporte para subir DataFrames directamente en formato Parquet

**Beneficios:**
- â˜ï¸ Almacenamiento escalable y duradero
- ğŸ” Credenciales seguras fuera del cÃ³digo
- ğŸ”„ Facilita integraciÃ³n con otros servicios AWS (Athena, Glue, Redshift)
- ğŸ“¦ Preparado para pipelines de producciÃ³n

```python
# Ejemplo de uso
from s3 import upload_file_to_s3, get_file_from_s3

# Subir DataFrame a S3
upload_file_to_s3(bucket_name, 'output/cleaned_data.parquet', df)

# Leer archivo desde S3
data = get_file_from_s3(bucket_name, 'data/raw_data.csv')
```

---

## ğŸ“Š Insights de Negocio Descubiertos

Al ejecutar el pipeline, se generaron los siguientes anÃ¡lisis:

| AnÃ¡lisis | DescripciÃ³n |
|----------|-------------|
| **Top 5 Clientes** | IdentificaciÃ³n de los 5 clientes con mayor gasto total |
| **Producto mÃ¡s vendido** | Ranking de productos por cantidad vendida |
| **EvoluciÃ³n mensual de ventas** | Tendencia temporal del total de ventas por mes |

---

## ğŸš€ CÃ³mo Ejecutar el Proyecto

### Requisitos Previos

```bash
Python 3.8+
pip install pandas numpy pyarrow boto3 python-dotenv
```

### InstalaciÃ³n

```bash
# Clonar repositorio
git clone [tu-repositorio]
cd etl

# Instalar dependencias
pip install -r requirements.txt
```

### ConfiguraciÃ³n de AWS S3

1. Crear un usuario IAM en AWS con permisos de S3
2. Generar Access Keys para el usuario
3. Crear un archivo `.env` en la raÃ­z del proyecto:

```env
AWS_ACCESS_KEY_ID=tu_access_key
AWS_SECRET_ACCESS_KEY=tu_secret_key
REGION=us-east-1
BUCKET_NAME=tu-bucket-name
```

> âš ï¸ **Importante:** Nunca subas el archivo `.env` a Git. AsegÃºrate de que estÃ© en `.gitignore`.

### EjecuciÃ³n

```bash
# Abrir y ejecutar el notebook
jupyter notebook etl.ipynb
```

---

## ğŸ“ Estructura del Proyecto

```
etl/
â”‚
â”œâ”€â”€ data/                 # Datos originales (CSV)
â”‚   â”œâ”€â”€ ecommerce_brands.csv
â”‚   â”œâ”€â”€ ecommerce_categories.csv
â”‚   â”œâ”€â”€ ecommerce_customers.csv
â”‚   â”œâ”€â”€ ecommerce_inventory.csv
â”‚   â”œâ”€â”€ ecommerce_order_items.csv
â”‚   â”œâ”€â”€ ecommerce_orders.csv
â”‚   â”œâ”€â”€ ecommerce_products.csv
â”‚   â”œâ”€â”€ ecommerce_promotions.csv
â”‚   â”œâ”€â”€ ecommerce_reviews.csv
â”‚   â”œâ”€â”€ ecommerce_suppliers.csv
â”‚   â””â”€â”€ ecommerce_warehouses.csv
â”‚
â”œâ”€â”€ output/               # Datos limpios (CSV y Parquet)
â”‚   â”œâ”€â”€ cleaned_*.csv
â”‚   â””â”€â”€ cleaned_*.parquet
â”‚
â”œâ”€â”€ etl.ipynb             # Notebook principal del pipeline
â”œâ”€â”€ config.py             # ConfiguraciÃ³n y variables de entorno
â”œâ”€â”€ s3.py                 # Funciones de integraciÃ³n con AWS S3
â”œâ”€â”€ .env                  # Variables de entorno (no incluido en Git)
â”œâ”€â”€ README.md             # Este archivo
â””â”€â”€ requirements.txt      # Dependencias
```

---

## ğŸ¯ Escalabilidad Futura

### Para datasets mÃ¡s grandes (100GB+):

1. **PySpark** para procesamiento distribuido
2. **Procesamiento incremental** (delta loads vs full refresh)
3. **Particionamiento** por fecha/categorÃ­a en Parquet
4. **OrquestaciÃ³n** con Airflow o Prefect
5. ~~**Cloud storage** (S3, Azure Blob)~~ âœ… **Implementado con AWS S3**

### Monitoreo y Calidad:

- Implementar Great Expectations para data quality
- Logging estructurado con Python logging
- Alertas automÃ¡ticas por fallos o anomalÃ­as
- Dashboard de mÃ©tricas con Grafana

---

## ğŸ§ª Validaciones Realizadas

El proyecto incluye validaciones de:
- âœ… ExploraciÃ³n de tipos de datos con `df.info()` y `df.describe()`
- âœ… IdentificaciÃ³n de valores nulos con `df.isnull().sum()`
- âœ… VerificaciÃ³n de ausencia de duplicados
- âœ… CorrecciÃ³n de tipos de datos (fechas, categorÃ­as, booleanos)
- âœ… Almacenamiento en formatos CSV y Parquet

---

## ğŸ“š Lecciones Aprendidas

### 1. La ExploraciÃ³n es CrÃ­tica
Casi apliquÃ© transformaciones incorrectas por no revisar los tipos de datos inicialmente. Ahora siempre inicio con `df.info()` y `df.describe()`.

### 2. Documentar Decisiones
En un mes olvidarÃ­a por quÃ© eliminÃ© ciertas filas. Ahora documento cada decisiÃ³n de limpieza con comentarios y logs.

### 3. Pensar en Escalabilidad
Parquet no solo es mÃ¡s pequeÃ±o, es significativamente mÃ¡s rÃ¡pido de leer. Esto importa cuando se escala a millones de registros.

### 4. Validar, Validar, Validar
Los datos nunca son perfectos. Implementar checks de calidad desde el inicio ahorra horas de debugging.

---

## ğŸ‘¤ Autor

**Tu Nombre**
- LinkedIn: [TomÃ¡s Amundrain](https://linkedin.com/in/tomasamundarain)
- GitHub: [tomy07417](https://github.com/tomy07417)
- Email: tomas07amunda@gmail.com

---

## ğŸ“„ Licencia

Este proyecto estÃ¡ bajo la Licencia MIT - ver el archivo [LICENSE](LICENSE) para mÃ¡s detalles.

---

## ğŸ™ Agradecimientos

Dataset inspirado en casos reales de e-commerce. Este proyecto forma parte de mi portafolio de Data Engineering.

---

<div align="center">
  <strong>Â¿Te gustÃ³ este proyecto?</strong><br>
  Dale una â­ si te sirviÃ³ de inspiraciÃ³n
</div>